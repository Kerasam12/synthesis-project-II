{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "Load and preprocess a dataset of Bitcoin transactions from CSV files, focusing on constructing adjacency matrices to represent the connections between transactions, along with their features and classifications for certain time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, start_ts, end_ts):\n",
    "\tclasses_csv = 'elliptic_txs_classes.csv'\n",
    "\tedgelist_csv = 'elliptic_txs_edgelist.csv'\n",
    "\tfeatures_csv = 'elliptic_txs_features.csv'\n",
    "\n",
    "\tclasses = pd.read_csv(os.path.join(data_dir, classes_csv), index_col = 'txId') # labels for the transactions i.e. 'unknown', '1', '2'\n",
    "\tedgelist = pd.read_csv(os.path.join(data_dir, edgelist_csv), index_col = 'txId1') # directed edges between transactions\n",
    "\tfeatures = pd.read_csv(os.path.join(data_dir, features_csv), header = None, index_col = 0) # features of the transactions\n",
    "\t\n",
    "\tprint('\\ncsvs leidos')\n",
    "\n",
    "\tnum_features = features.shape[1]\n",
    "\tnum_tx = features.shape[0]\t\n",
    "\ttotal_tx = list(classes.index)\n",
    "\n",
    "\t# select only the transactions which are labelled\n",
    "\tlabelled_classes = classes[classes['class'] != 'unknown']\n",
    "\tlabelled_tx = list(labelled_classes.index)\n",
    "\n",
<<<<<<< HEAD
    "\t"
=======
    "\tprint('\\nonly the transactions which are labelled selected')\n",
    "\n",
    "\t# to calculate a list of adjacency matrices for the different timesteps\n",
    "\tadj_mats = []\n",
    "\tfeatures_labelled_ts = []\n",
    "\tclasses_ts = []\n",
    "\tnum_ts = 49 # number of timestamps from the paper\n",
    "\n",
    "\t# Convert total_tx to a mapping of transaction ID to matrix index\n",
    "\ttx_to_index = {tx_id: idx for idx, tx_id in enumerate(total_tx)}\n",
    "\n",
    "\tprint('\\nConvert total_tx to a mapping of transaction ID to matrix index')\n",
    "\n",
    "\t#For every timestep between start and end, it prepares an adj matrix\n",
    "\n",
    "\tfor ts in range(start_ts, end_ts):\n",
    "    \t\n",
    "\t\tfeatures_ts = features[features[1] == ts + 1]\n",
    "            \n",
    "\t\ttx_ts = list(features_ts.index)\n",
    "        \n",
    "\t\tlabelled_tx_ts = [tx for tx in tx_ts if tx in set(labelled_tx)]\n",
    "\n",
    "        # Using lil_matrix for easier incremental construction\n",
    "        \n",
    "\t\tadj_mat = lil_matrix((num_tx, num_tx), dtype=int)\n",
    "\n",
    "        \n",
    "\t\tedgelist_labelled_ts = edgelist.loc[edgelist.index.intersection(labelled_tx_ts).unique()]\n",
    "        \n",
    "\t\tfor i in range(edgelist_labelled_ts.shape[0]):\n",
    "            \n",
    "\t\t\ttx1 = edgelist_labelled_ts.index[i]\n",
    "            \n",
    "\t\t\ttx2 = edgelist_labelled_ts.iloc[i]['txId2']\n",
    "            \n",
    "\t\t\tif tx1 in tx_to_index and tx2 in tx_to_index:  # Check if both tx exist in the mapping\n",
    "                \n",
    "\t\t\t\tadj_mat[tx_to_index[tx1], tx_to_index[tx2]] = 1\n",
    "\n",
    "\t\tprint('end of loop')\n",
    "\n",
    "        # Convert back to csr_matrix for efficient arithmetic and slicing\n",
    "\n",
    "\t\tadj_mat_csr = adj_mat.tocsr()\n",
    "        \n",
    "\t\tadj_mats.append(adj_mat_csr)\n",
    "\n",
    "\t\tprint('adjacency matrix done')\n",
    "\n",
    "        # Filter features and classes for labelled transactions of this timestep\n",
    "        \n",
    "\t\tfeatures_labelled_ts.append(features.loc[labelled_tx_ts])\n",
    "\n",
    "\t\tprint('features labelled done')\n",
    "        \n",
    "\t\tclasses_ts.append(classes.loc[labelled_tx_ts])\n",
    "\n",
    "\t\tprint('classes ts done')\n",
    "\n",
    "\treturn adj_mats, features_labelled_ts, classes_ts"
>>>>>>> 9186996d7c4cfab9d8af69537f7b3464ec94e8c8
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "csvs leidos\n",
      "\n",
      "only the transactions which are labelled selected\n",
      "\n",
      "Convert total_tx to a mapping of transaction ID to matrix index\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n",
      "end of loop\n",
      "adjacency matrix done\n",
      "features labelled done\n",
      "classes ts done\n"
     ]
    }
   ],
   "source": [
    "dir = r\"C:\\Users\\User\\Desktop\\UAB\\3rd-year\\2nd-semester\\synthesis project II\\elliptic_bitcoin_dataset\"\n",
    "dataSet = load_data(dir, 0, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type adj_mats:\n",
      " <class 'list'>\n",
      "type adj_mats[0]:\n",
      " <class 'scipy.sparse._csr.csr_matrix'>\n",
      "features_labelled_ts:\n",
      " <class 'list'>\n",
      "classes_ts:\n",
      " <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#print(dataSet)\n",
    "\n",
    "adj_mats, features_labelled_ts, classes_ts = dataSet\n",
    "\n",
    "#print('adj_mats[0]:\\n', adj_mats[0])\n",
    "print('type adj_mats:\\n', type(adj_mats))\n",
    "print('type adj_mats[0]:\\n', type(adj_mats[0]))\n",
    "\n",
    "print('features_labelled_ts:\\n', type(features_labelled_ts))\n",
    "\n",
    "print('classes_ts:\\n', type(classes_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation  = 'relu', skip = False, skip_in_features = None):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.DoubleTensor(in_features, out_features))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        \n",
    "        self.set_act = False\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "            self.set_act = True\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim = 1)\n",
    "            self.set_act = True\n",
    "        else:\n",
    "            self.set_act = False\n",
    "            raise ValueError(\"activations supported are 'relu' and 'softmax'\")\n",
    "            \n",
    "        self.skip = skip\n",
    "        if self.skip:\n",
    "            if skip_in_features == None:\n",
    "                raise ValueError(\"pass input feature size of the skip connection\")\n",
    "            self.W_skip = torch.nn.Parameter(torch.DoubleTensor(skip_in_features, out_features)) \n",
    "            nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, A, H_in, H_skip_in = None):\n",
    "        # A must be an n x n matrix as it is an adjacency matrix\n",
    "        # H is the input of the node embeddings, shape will n x in_features\n",
    "        self.A = A\n",
    "        self.H_in = H_in\n",
    "        A_ = torch.add(self.A, torch.eye(self.A.shape[0]).double())\n",
    "        D_ = torch.diag(A_.sum(1))\n",
    "        # since D_ is a diagonal matrix, \n",
    "        # its root will be the roots of the diagonal elements on the principle diagonal\n",
    "        # since A is an adjacency matrix, we are only dealing with positive values \n",
    "        # all roots will be real\n",
    "        D_root_inv = torch.inverse(torch.sqrt(D_))\n",
    "        A_norm = torch.mm(torch.mm(D_root_inv, A_), D_root_inv)\n",
    "        # shape of A_norm will be n x n\n",
    "        \n",
    "        H_out = torch.mm(torch.mm(A_norm, H_in), self.W)\n",
    "        # shape of H_out will be n x out_features\n",
    "        \n",
    "        if self.skip:\n",
    "            H_skip_out = torch.mm(H_skip_in, self.W_skip)\n",
    "            H_out = torch.add(H_out, H_skip_out)\n",
    "        \n",
    "        if self.set_act:\n",
    "            H_out = self.activation(H_out)\n",
    "            \n",
    "        return H_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_2layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, skip = False):\n",
    "        super(GCN_2layer, self).__init__()\n",
    "        self.skip = skip\n",
    "        \n",
    "        self.gcl1 = GraphConv(in_features, hidden_features)\n",
    "        \n",
    "        if self.skip:\n",
    "            self.gcl_skip = GraphConv(hidden_features, out_features, activation = 'softmax', skip = self.skip,\n",
    "                                  skip_in_features = in_features)\n",
    "        else:\n",
    "            self.gcl2 = GraphConv(hidden_features, out_features, activation = 'softmax')\n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        out = self.gcl1(A, X)\n",
    "        if self.skip:\n",
    "            out = self.gcl_skip(A, out, X)\n",
    "        else:\n",
    "            out = self.gcl2(A, out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22312\\3243306931.py:12: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n"
     ]
    }
   ],
   "source": [
    "num_features = 166\n",
    "num_classes = 2\n",
    "num_ts = 49\n",
    "epochs = 15\n",
    "lr = 0.001\n",
    "max_train_ts = 34\n",
    "train_ts = np.arange(max_train_ts)\n",
    "\n",
    "# 0 - illicit, 1 - licit\n",
    "labels_ts = []\n",
    "for c in classes_ts:\n",
    "    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n",
    "\n",
    "gcn = GCN_2layer(num_features, 100, num_classes)\n",
    "train_loss = nn.CrossEntropyLoss(weight = torch.DoubleTensor([0.7, 0.3]))\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 166087221444 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m gcn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_loss(out, L)\n\u001b[0;32m     19\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype_as(L)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mGCN_2layer.forward\u001b[1;34m(self, A, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, A, X):\n\u001b[1;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcl1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[0;32m     17\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcl_skip(A, out, X)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[1;34m(self, A, H_in, H_skip_in)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m A\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_in \u001b[38;5;241m=\u001b[39m H_in\n\u001b[1;32m---> 30\u001b[0m A_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdouble())\n\u001b[0;32m     31\u001b[0m D_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(A_\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# since D_ is a diagonal matrix, \u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# its root will be the roots of the diagonal elements on the principle diagonal\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# since A is an adjacency matrix, we are only dealing with positive values \u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# all roots will be real\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 166087221444 bytes."
     ]
    }
   ],
   "source": [
    "for ts in train_ts:\n",
    "    A = adj_mats[ts].tocoo()  # Convert to COOrdinate\n",
    "    indices = torch.tensor(np.vstack((A.row, A.col)), dtype=torch.long)\n",
    "    values = torch.tensor(A.data, dtype=torch.float32)\n",
    "    shape = torch.Size(A.shape)\n",
    "    A_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    \n",
    "    #A = torch.tensor(adj_mats[ts].values)\n",
    "    X = torch.tensor(features_labelled_ts[ts].values)\n",
    "    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n",
    "    for ep in range(epochs):\n",
    "        t_start = time.time()\n",
    "        \n",
    "        gcn.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = gcn(A, X)\n",
    "\n",
    "        loss = train_loss(out, L)\n",
    "        train_pred = out.max(1)[1].type_as(L)\n",
    "        acc = (train_pred.eq(L).double().sum())/L.shape[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sys.stdout.write(\"\\r Epoch %d/%d Timestamp %d/%d training loss: %f training accuracy: %f Time: %s\"\n",
    "                         %(ep, epochs, ts, max_train_ts, loss, acc, time.time() - t_start)\n",
    "                        )\n",
    "\n",
    "torch.save(gcn.state_dict(), str(os.path.join(r\"C:\\Users\\User\\Desktop\\UAB\\3rd-year\\2nd-semester\\synthesis project II\\synthesis-project-II\\Paula\\GCN_model\", \"gcn_weights.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "test_ts = np.arange(14)\n",
    "adj_mats, features_labelled_ts, classes_ts = load_data(dir, 35, 49)\n",
    "\n",
    "# 0 - illicit, 1 - licit\n",
    "labels_ts = []\n",
    "for c in classes_ts:\n",
    "    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n",
    "\n",
    "gcn = GCN_2layer(num_features, 100, num_classes)\n",
    "gcn.load_state_dict(torch.load(os.path.join(\"./modelDir\", \"gcn_weights.pth\")))\n",
    "\n",
    "# Testing\n",
    "test_accs = []\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "test_f1s = []\n",
    "\n",
    "for ts in test_ts:\n",
    "    A = torch.tensor(adj_mats[ts].values)\n",
    "    X = torch.tensor(features_labelled_ts[ts].values)\n",
    "    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n",
    "    \n",
    "    gcn.eval()\n",
    "    test_out = gcn(A, X)\n",
    "    \n",
    "    test_pred = test_out.max(1)[1].type_as(L)\n",
    "    t_acc = (test_pred.eq(L).double().sum())/L.shape[0]\n",
    "    test_accs.append(t_acc.item())\n",
    "    test_precisions.append(precision_score(L, test_pred))\n",
    "    test_recalls.append(recall_score(L, test_pred))\n",
    "    test_f1s.append(f1_score(L, test_pred))\n",
    "\n",
    "acc = np.array(test_accs).mean()\n",
    "prec = np.array(test_precisions).mean()\n",
    "rec = np.array(test_recalls).mean()\n",
    "f1 = np.array(test_f1s).mean()\n",
    "\n",
    "print(\"GCN - averaged accuracy: {}, precision: {}, recall: {}, f1: {}\".format(acc, prec, rec, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
